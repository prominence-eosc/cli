#!/usr/bin/env python

from __future__ import print_function
import argparse
import base64
from collections import OrderedDict
import errno
import json
import os
import re
import sys
import time
import uuid
import requests

from prominence import auth
from prominence import ProminenceJob
from prominence import ProminenceTask
from prominence import ProminenceClient
from prominence import __version__

def elapsed(job):
    """
    Print elapsed job runtime in a nice way
    """
    if 'startTime' in job['events']:
        if 'endTime' in job['events']:
            elapsed_time = job['events']['endTime'] - job['events']['startTime']
        else:
            elapsed_time = time.time() - job['events']['startTime']
        days = int(elapsed_time/86400)
        time_fmt = '%H:%M:%S'
        return '%d+%s' % (days, time.strftime(time_fmt, time.gmtime(elapsed_time)))

    return ''

def datetime_format(epoch):
    """
    Convert a unix epoch in a formatted date/time string
    """
    datetime_fmt = '%Y-%m-%dT%H:%M:%S'
    return time.strftime(datetime_fmt, time.gmtime(epoch))

def print_json(content, transform=False, detail=False, resource='job'):
    """
    Print JSON in a nice way
    """
    if transform:
        content = transform_item_list(content, detail, resource)
    print(json.dumps(content, indent=2))

def image_name(name):
    """
    Extract container image name for display purposes only
    """
    if 'http' in name:
        name = os.path.basename(name)
        name = name[:name.find('?')]
    return name

def list_jobs(jobs):
    """
    Print list of jobs
    """

    # Firstly determine column widths
    width_id = 2
    width_name = 4
    width_created = 19
    width_status = 6
    width_elapsed = 10
    width_container = 5
    width_cmd = 3

    for job in jobs:
        my_cmd = ''
        if 'cmd' in job['tasks'][0]:
            my_cmd = job['tasks'][0]['cmd']

        width_id_current = len(str(job['id']))
        width_name_current = len(job['name'])
        width_status_current = len(job['status'])
        width_container_current = len(image_name(job['tasks'][0]['image']))
        width_cmd_current = len(my_cmd)

        if width_id_current > width_id:
            width_id = width_id_current
        if width_name_current > width_name:
            width_name = width_name_current
        if width_status_current > width_status:
            width_status = width_status_current
        if width_container_current > width_container:
            width_container = width_container_current
        if width_cmd_current > width_cmd:
            width_cmd = width_cmd_current

    # Print headings
    print('%s   %s   %s   %s   %s   %s   %s' % ('ID'.ljust(width_id),
                                                'NAME'.ljust(width_name),
                                                'CREATED'.ljust(width_created),
                                                'STATUS'.ljust(width_status),
                                                'ELAPSED'.ljust(width_elapsed),
                                                'IMAGE'.ljust(width_container),
                                                'CMD'.ljust(width_cmd)))

    # Print jobs
    for job in jobs:
        my_cmd = ''
        if 'cmd' in job['tasks'][0]:
            my_cmd = job['tasks'][0]['cmd']
        print('%s   %s   %s   %s   %s   %s   %s' % (str(job['id']).ljust(width_id),
                                                    job['name'].ljust(width_name),
                                                    datetime_format(job['events']['createTime']).ljust(width_created),
                                                    job['status'].ljust(width_status),
                                                    elapsed(job).ljust(width_elapsed),
                                                    image_name(job['tasks'][0]['image']).ljust(width_container),
                                                    my_cmd.ljust(width_cmd)))

def list_workflows(workflows):
    """
    Print list of workflows
    """

    # Firstly determine column widths
    width_id = 2
    width_name = 4
    width_created = 19
    width_status = 6
    width_elapsed = 10
    width_progress = 8

    for workflow in workflows:
        width_id_current = len(str(workflow['id']))
        width_name_current = len(workflow['name'])
        width_status_current = len(workflow['status'])
        width_progress_current = len('%d/%d' % (workflow['progress']['done'], workflow['progress']['total']))

        if width_id_current > width_id:
            width_id = width_id_current
        if width_name_current > width_name:
            width_name = width_name_current
        if width_status_current > width_status:
            width_status = width_status_current
        if width_progress_current > width_progress:
            width_progress = width_progress_current

    # Print headings
    print('%s   %s   %s   %s   %s   %s' % ('ID'.ljust(width_id),
                                           'NAME'.ljust(width_name),
                                           'CREATED'.ljust(width_created),
                                           'STATUS'.ljust(width_status),
                                           'ELAPSED'.ljust(width_elapsed),
                                           'PROGRESS'.ljust(width_progress)))

    # Print workflows
    for workflow in workflows:
        print('%s   %s   %s   %s   %s   %s' % (str(workflow['id']).ljust(width_id),
                                               workflow['name'].ljust(width_name),
                                               datetime_format(workflow['events']['createTime']).ljust(width_created),
                                               workflow['status'].ljust(width_status),
                                               ''.ljust(width_elapsed),
                                               ('%d/%d' % (workflow['progress']['done'], workflow['progress']['total'])).ljust(width_progress)))

def transform_job(job, detail):
    """
    Transform a job into the required format for printing
    """
    job_t = OrderedDict()
    job_t['id'] = job['id']
    if job['name'] != '' or not detail:
        job_t['name'] = job['name']

    job_t['status'] = job['status']

    if detail and 'statusReason' in job:
        job_t['statusReason'] = job['statusReason']

    if detail:
        if 'storage' in job:
            job_t['storage'] = job['storage']
        job_t['resources'] = job['resources']
        if 'labels' in job:
            job_t['labels'] = job['labels']
        if 'artifacts' in job:
            job_t['artifacts'] = job['artifacts']
        if 'inputFiles' in job:
            job_t['inputFiles'] = job['inputFiles']
        if 'outputFiles' in job:
            job_t['outputFiles'] = job['outputFiles']
        if 'outputDirs' in job:
            job_t['outputDirs'] = job['outputDirs']

    job_t['tasks'] = job['tasks']

    if detail:
        if 'preemptible' in job:
            job_t['preemptible'] = True

    events = OrderedDict()
    if 'events' in job:
        if 'createTime' in job['events']:
            if detail:
                events['createTime'] = datetime_format(job['events']['createTime'])
            else:
                events['createTime'] = job['events']['createTime']

    if 'startTime' in job['events']:
        if detail:
            events['startTime'] = datetime_format(job['events']['startTime'])
        else:
            events['startTime'] = job['events']['startTime']
    if 'endTime' in job['events']:
        if detail:
            events['endTime'] = datetime_format(job['events']['endTime'])
        else:
            events['endTime'] = job['events']['endTime']
    job_t['events'] = events

    execution = OrderedDict()
    if 'execution' in job:
        if 'site' in job['execution']:
            execution['site'] = job['execution']['site']
        if 'tasks' in job['execution']:
            tasks = []
            for task in job['execution']['tasks']:
                task_t = {}
                if 'exitCode' in task:
                    task_t['exitCode'] = task['exitCode']
                if 'imagePullTime' in task:
                    task_t['imagePullTime'] = float('%.4g' % task['imagePullTime'])
                if 'wallTimeUsage' in task:
                    task_t['wallTimeUsage'] = float('%.4g' % task['wallTimeUsage'])
                if 'cpuTimeUsage' in task:
                    task_t['cpuTimeUsage'] = float('%.4g' % task['cpuTimeUsage'])
                tasks.append(task_t)

            execution['tasks'] = tasks
        job_t['execution'] = execution

    return job_t

def transform_workflow(workflow, detail):
    """
    Transform a workflow into the required format for printing
    """
    workflow_t = OrderedDict()
    workflow_t['id'] = workflow['id']
    if workflow['name'] != '' or not detail:
        workflow_t['name'] = workflow['name']

    workflow_t['status'] = workflow['status']

    if detail and 'statusReason' in workflow:
        workflow_t['statusReason'] = workflow['statusReason']

    if detail:
        if 'storage' in workflow:
            workflow_t['storage'] = workflow['storage']
        workflow_t['jobs'] = workflow['jobs']
        if 'dependencies' in workflow:
            workflow_t['dependencies'] = workflow['dependencies']

    events = OrderedDict()
    if 'events' in workflow:
        if 'createTime' in workflow['events']:
            if detail:
                events['createTime'] = datetime_format(workflow['events']['createTime'])
            else:
                events['createTime'] = workflow['events']['createTime']

    if 'startTime' in workflow['events']:
        if detail:
            events['startTime'] = datetime_format(workflow['events']['startTime'])
        else:
            events['startTime'] = workflow['events']['startTime']
    if 'endTime' in workflow['events']:
        if detail:
            events['endTime'] = datetime_format(workflow['events']['endTime'])
        else:
            events['endTime'] = workflow['events']['endTime']
    workflow_t['events'] = events

    if 'progress' in workflow:
        workflow_t['progress'] = workflow['progress']

    return workflow_t

def transform_item_list(result, detail, resource):
    """
    Transform a job/workflow list into the required format ordered by id
    """
    if 'job' in resource:
        items = [transform_job(job, detail) for job in result]
    else:
        items = [transform_workflow(workflow, detail) for workflow in result]
    return sorted(items, key=lambda k: int(k['id']))

def command_register(args):
    """
    Obtain a client id and secret from the OIDC provider
    """
    try:
        auth.register_client()
    except Exception as e:
        print('Error:', e)
        exit(1)

def command_login(args):
    """
    Obtain token from OIDC provider
    """
    try:
        auth.authenticate_user(False)
    except Exception as e:
        print('Error:', e)
        exit(1)

def command_list(args):
    """
    List running/idle jobs/workflows or completed jobs/workflows
    """
    completed = False
    if args.completed:
        completed = True
    all = False
    if args.all:
        all = True
    num = None
    if args.num:
        num = args.num
    constraint = None
    if args.constraint:
        constraint = args.constraint

    try:
        client = ProminenceClient()
        if args.resource == 'jobs':
            data = client.list_jobs(completed, all, num, constraint)
        else:
            data = client.list_workflows(completed, all, num, constraint)
    except Exception as e:
        print('Error:', e)
        exit(1)
        
    if args.resource == 'jobs':
        list_jobs(transform_item_list(data, False, 'job'))
    else:
        list_workflows(transform_item_list(data, False, 'workflow'))

def command_describe(args):
    """
    Describe a specific job/workflow
    """

    try:
        client = ProminenceClient()
        if args.resource == 'job':
            data = client.describe_job(args.id)
        else:
            data = client.describe_workflow(args.id)
    except Exception as e:
        print('Error:', e)
        exit(1)

    print_json(data, transform=True, detail=True, resource=args.resource)
    exit(0)

def command_upload(args):
    """
    Upload a file to transient storage
    """
    if args.name is None:
        print('Error: a name must be specified')
        exit(1)
    if args.filename is None:
        print('Error: a filename must be specified')
        exit(1)

    try:
        client = ProminenceClient()
        response = client.upload(args.name, args.filename)
    except Exception as e:
        print('Error:', e)
        exit(1)

    print('Success')

def command_download(args):
    """
    Download output files and directories
    """
    try:
        client = ProminenceClient()
        if args.constraint:
            jobs = client.list(False, True, 0, args.constraint)
        else:
            if not args.id:
                print('Error: A job id must be given if a constraint if not specified')
                exit(1)
            jobs = client.describe_job(args.id, True)
    except Exception as e:
        print('Error:', e)
        exit(1)

    for job in jobs:
        if ('outputFiles' in job or 'outputDirs' in job) and job['status'] == 'completed':
            # Create per-job directory if necessary & set path for files
            path = './'
            if args.dir:
                path = './%d/' % job['id']
                try:
                    os.mkdir(path)
                except OSError as exc:
                    if exc.errno != errno.EEXIST:
                        print('WARNING: skipping job %d as job directory cannot be created' % job['id'])
                        continue
                    else:
                        pass

            files_and_dirs = []
            if 'outputFiles' in job:
                files_and_dirs += job['outputFiles']
            if 'outputDirs' in job:
                files_and_dirs += job['outputDirs']

            for pair in files_and_dirs:
                file_name = os.path.basename(pair['name'])
                if 'outputDirs' in job:
                    if pair in job['outputDirs']:
                        file_name = file_name + '.tgz'

                url = pair['url']

                if os.path.isfile(path + file_name) and not args.force:
                    print('WARNING: skipping "%s" from job %d as file already exists and force option not specified' % (file_name, job['id']))
                    continue

                response = requests.get(url, stream=True)
                total_length = response.headers.get('content-length')

                if response.status_code != 200:
                    print('WARNING: skipping "%s" from job %d as it does not exist' % (file_name, job['id']))
                    continue

                with open(path + file_name, 'wb') as file_download:
                    print('Downloading file "%s" from job %d' % (file_name, job['id']))
                    if total_length is None:
                        file_download.write(response.content)
                    else:
                        downloaded = 0
                        total_length = int(total_length)
                        for data in response.iter_content(chunk_size=4096):
                            downloaded += len(data)
                            file_download.write(data)
                            done = int(50 * downloaded / total_length)
                            sys.stdout.write("\r[%s%s]" % ('=' * done, ' ' * (50-done)))
                            sys.stdout.flush()
                print('')

def command_delete(args):
    """
    Delete a job
    """
    try:
        client = ProminenceClient()
        if args.resource == 'job':
            client.delete_job(args.id)
        else:
            client.delete_workflow(args.id)
    except Exception as e:
        print('Error:', e)
        exit(1)

    print('Success')
    exit(0)

def command_stdout(args):
    """
    Get standard output for a specific job/workflow
    """
    try:
        client = ProminenceClient()
        if args.job:
            print(client.stdout_workflow(args.id, args.job))
        else:
            print(client.stdout_job(args.id))
    except Exception as e:
        print('Error:', e)
        exit(1)

def command_stderr(args):
    """
    Get standard error for a specific job/workflow
    """
    try:
        client = ProminenceClient()
        if args.job:
            print(client.stderr_workflow(args.id, args.job))
        else:
            print(client.stderr_job(args.id))
    except Exception as e:
        print('Error:', e)
        exit(1)

def command_run(args):
    """
    Create a job from a JSON file or URL
    """
    if args.file.startswith('http://') or args.file.startswith('https://'):
        try:
            response = requests.get(args.file)
        except requests.exceptions.RequestException:
            print('Error getting URL due to: %s' % reponse.text)
            exit(1)
        if response.status_code == 200:
            data = response.json()
        else:
            print('Got status %d from URL with message: %s' % (response.status_code, response.text))
            exit(1)
    else:
        try:
            with open(args.file) as json_file:
                data = json.load(json_file)
        except IOError as err:
            print('Error: %s' % err)
            exit(1)
        except ValueError as err:
            print('Error: %s' % err)
            exit(1)

    try:
        client = ProminenceClient()
        if 'dependencies' in data:
            id = client.create_workflow_from_json(data)
            resource = 'Workflow'
        else:
            id = client.create_job_from_json(data)
            resource = 'Job'
    except Exception as e:
        print('Error:', e)
        exit(1)

    print('%s created with id %d' % (resource, id))
       
def command_create(args):
    """
    Create a job
    """
    job = ProminenceJob()
    task = ProminenceTask()

    task.image = args.image
    job.memory = args.memory
    job.cpus = args.cpus
    job.nodes = args.nodes
    job.disk = args.disk
    job.name = args.name

    # MPI processes per node
    if args.openmpi or args.mpich:
        if args.ppn == 0:
            task.procs_per_node = job.cpus
        else:
            task.procs_per_node = args.ppn

    # Working directory
    if args.workdir:
        task.workdir = args.workdir

    # Walltine limit
    job.walltime = args.walltime

    # Container runtime - use singularity by default but use udocker if the user has
    # specified a tarball using a URL
    if args.runtime:
        task.runtime = args.runtime
    else:
        if re.match(r'^http', args.image) and '.tar' in args.image:
            task.runtime = 'udocker'
        else:
            task.runtime = 'singularity'

    # Job type
    if args.openmpi:
        task.type = 'openmpi'
    elif args.mpich:
        task.type = 'mpich'

    # If multiple nodes are specified need to specify MPI type
    if job.nodes > 1 and task.type != 'openmpi' and task.type != 'mpich':
        print('Error: more than one node has been requested but MPI has not been specified')
        exit(1)

    # Optional command to run
    if args.command:
        task.cmd = args.command

    # Output files
    if args.outputfile:
        job.output_files = args.outputfile

    # Output directories
    if args.outputdir:
        job.output_dirs = args.outputdir

    # Files to be fetched
    if args.artifact:
        job.artifacts = []
        for file in args.artifact:
            artifact = {}
            artifact['url'] = file
            if ':' in file and ('https:' not in file or ('https:' in file and file.count(':') > 1)):
                artifact['url'] = file.split(':')[0]
                artifact['mountpoint'] = '%s:%s' % (file.split(':')[1], file.split(':')[2])
            else:
                artifact['url'] = file
            job.artifacts.append(artifact)

    # Files to be uploaded
    if args.inputfile:
        inputs = []
        for filename in args.inputfile:
            if os.path.isfile(filename):
                if os.path.getsize(filename) < 1000000:
                    with open(filename, 'rb') as input_file:
                        inputs.append({'filename':filename, 'content':base64.b64encode(input_file.read()).decode("utf-8")})
                else:
                    print('Error: Input file size too large')
                    exit(1)
            else:
                print('Error: File "%s" does not exist' % filename)
                exit(1)
        job.inputs = inputs

    # Environment variables
    if args.env:
        env = {}
        for pair in args.env:
            if '=' in pair:
                items = pair.split('=')
                env[items[0]] = items[1]
        task.env = env

    # Metadata
    if args.label:
        labels = {}
        for pair in args.label:
            if '=' in pair:
                items = pair.split('=')
                labels[items[0]] = items[1]
        job.labels = labels

    # Constraints
    if args.constraints:
        job.constraints = json.loads(args.constraints)

    # Storage
    if args.storage:
        try:
            with open(args.storage) as json_file:
                storage = '{%s}' % json_file.read()
        except IOError as err:
            print('Error: %s' % err)
            exit(1)
        except ValueError as err:
            print('Error: %s' % err)
            exit(1)
        storage = json.loads(storage)
        if 'storage' in storage:
            job.storage = storage['storage']

    # Preemptible
    if args.preemptible:
        job.preemptible = args.preemptible

    # Add task to job
    job.tasks = [task]

    # Print JSON description of job if requested
    if args.dryrun:
        print_json(job.to_json())
        exit(0)

    try:
        client = ProminenceClient()
        id = client.create_job(job)
    except Exception as e:
        print('Error:', e)
        exit(1)

    print('Job created with id %d' % id)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Prominence - \
                                     run jobs in containers across clouds')
    subparsers = parser.add_subparsers(help='sub-command help')
    
    # Create the parser for the "register" command
    parser_register = subparsers.add_parser('register', help='Register as a client with the OIDC server')
    parser_register.set_defaults(func=command_register)

    # Create the parser for the "login" command
    parser_login = subparsers.add_parser('login', help='Get a token from the OIDC server')
    parser_login.set_defaults(func=command_login)

    # Create the parser for the "run" command
    parser_run = subparsers.add_parser('run', help='Create a job or workflow from JSON in a file or URL')
    parser_run.add_argument('file', help='JSON filename or URL containing JSON')
    parser_run.set_defaults(func=command_run)

    # Create the parser for the "create" command
    parser_create = subparsers.add_parser('create', help='Create a job')
    parser_create.add_argument('--name', dest='name', default='',
                               help='Job name.')
    parser_create.add_argument('--memory', dest='memory', default=1, type=int,
                               help='Memory in GB per node.')
    parser_create.add_argument('--cpus', dest='cpus', default=1, type=int,
                               help='Cores per node.')
    parser_create.add_argument('--nodes', dest='nodes', default=1, type=int,
                               help='Number of nodes.')
    parser_create.add_argument('--procs-per-node', dest='ppn', default=0, type=int,
                               help='Number of MPI processes to launch per node. By default this \
                                     will be the number of CPU cores requested per node but it can \
                                     be changed if necessary.')
    parser_create.add_argument('--disk', dest='disk', default=10, type=int,
                               help='Size of disk containing the job\'s scratch directory. For \
                                     multi-node jobs it will be shared across each of the nodes. \
                                     By default a 10 GB disk will be used.')
    parser_create.add_argument('--walltime', dest='walltime', type=int,
                               help='Walltime limit in minutes. If the job is still running after \
                                     this time it will be killed.')
    parser_create.add_argument('--openmpi', dest='openmpi', default=False, action='store_true',
                               help="Specify that this is an OpenMPI job.")
    parser_create.add_argument('--mpich', dest='mpich', default=False, action='store_true',
                               help="Specify that this is an MPICH job.")
    parser_create.add_argument('--artifact', dest='artifact', action='append',
                               help='A URL to be transferred to the job. Archives will be \
                                     automatically unpacked/extracted. Optionally, for the \
                                     case of a tarball or zip archive, a directory name and \
                                     mount point can be specified by using the format \
                                     "<URL>:<directory>:<mountpoint>". The mount point must be \
                                     an absolute path. \
                                     This option can be specified multiple times.')
    parser_create.add_argument('--input', dest='inputfile', action='append',
                               help='Full path to a file on the current host to be \
                                     uploaded and made available to the job. This option \
                                     can be specified multiple times to set multiple output files.')
    parser_create.add_argument('--output', dest="outputfile", action='append',
                               help='An output file to be copied to transient storage. This option \
                                     can be specified multiple times to set multiple output files.')
    parser_create.add_argument('--outputdir', dest="outputdir", action='append',
                               help='A directory to be copied to transient storage. This option \
                                     can be specified multiple times to set multiple directories.')
    parser_create.add_argument('--workdir', dest='workdir',
                               help='Set the current working directory.')
    parser_create.add_argument('--env', dest='env', action='append',
                               help='Specify environment variables in the form name=value. \
                                     This option can be specified multiple times to set \
                                     multiple environment variables.')
    parser_create.add_argument('--label', dest='label', action='append',
                               help='Set metadata in the form key=value. This option can \
                                     be specified multiple times to set multiple labels.')
    parser_create.add_argument('--runtime', dest='runtime',
                               choices=['singularity', 'udocker'],
                               help='Container runtime, either singularity or udocker. The default \
                                     is singularity.')
    parser_create.add_argument('--preemptible', dest='preemptible', default=False, action='store_true',
                               help='Specify that this job is preemptible')
    parser_create.add_argument('--constraints', dest='constraints',
                               help='Specify constraints')
    parser_create.add_argument('--storage', dest='storage',
                               help='Filename specifying storage details')
    parser_create.add_argument('--dry-run', dest='dryrun', default=False, action='store_true',
                               help='Print json to stdout but do not actually create job.')
    parser_create.add_argument('image', help='Container image')
    parser_create.add_argument('command', nargs='?',
                               help='Command to run in the container. If you need to specify \
                                     arguments, put the combined command and arguments inside quotes.')
    parser_create.set_defaults(func=command_create)

    # Create the parser for the "list" command
    parser_list = subparsers.add_parser('list', help='List jobs or workflows')
    parser_list.add_argument('--completed', dest='completed', default=False,
                             help='List completed jobs/workflows', action='store_true')
    parser_list.add_argument('-n', '--last', dest='num', default=1, type=int,
                             help='Number of completed jobs/workflows to return')
    parser_list.add_argument('--constraint', dest='constraint', action='append',
                             help='Constraint of the form key=value')
    parser_list.add_argument('-a', '--all', dest='all', default=False,
                             help='List jobs/workflows in all states', action='store_true')
    parser_list.add_argument('resource', help='Resource type', default='jobs', nargs='?',
                             choices=['jobs', 'workflows'])
    parser_list.set_defaults(func=command_list)

    # Create the parser for the "describe" command
    parser_describe = subparsers.add_parser('describe', help='Describe a job or workflow')
    parser_describe.add_argument('resource', help='Resource type', default='job', nargs='?',
                                 choices=['job', 'workflow'])
    parser_describe.add_argument('id', help='Job id', type=int)
    parser_describe.set_defaults(func=command_describe)

    # Create the parser for the "delete" command
    parser_delete = subparsers.add_parser('delete', help='Delete a job or workflow')
    parser_delete.add_argument('resource', help='Resource type', default='job', nargs='?',
                               choices=['job', 'workflow'])
    parser_delete.add_argument('id', help='Job/workflow id', type=int)
    parser_delete.set_defaults(func=command_delete)

    # Create the parser for the "upload" command
    parser_upload = subparsers.add_parser('upload', help='Upload a file to transient storage')
    parser_upload.add_argument('--name', dest='name', help='Name to be used by jobs to identity the file')
    parser_upload.add_argument('--filename', dest='filename', help='Local filename')
    parser_upload.set_defaults(func=command_upload)

    # Create the parser for the "download" command
    parser_download = subparsers.add_parser('download', help='Download output files from a completed job or workflow')
    parser_download.add_argument('--constraint', dest='constraint', action='append',
                                 help='Constraint of the form key=value')
    parser_download.add_argument('--force', dest='force', default=False,
                                 help='Force overwrite of existing file', action='store_true')
    parser_download.add_argument('--dir', dest='dir', default=False,
                                 help='Save output files in a directory named by the job id', action='store_true')
    parser_download.add_argument('id', help='Job id', type=int)
    parser_download.set_defaults(func=command_download)

    # Create the parser for the "stdout" command
    parser_stdout = subparsers.add_parser('stdout', help='Get standard output from a running or completed job')
    parser_stdout.add_argument('id', help='Job or workflow id', type=int)
    parser_stdout.add_argument('job', help='Job name', nargs='?')
    parser_stdout.set_defaults(func=command_stdout)

    # Create the parser for the "stderr" command
    parser_stderr = subparsers.add_parser('stderr', help='Get standard error from a running or completed job')
    parser_stderr.add_argument('id', help='Job or workflow id', type=int)
    parser_stderr.add_argument('job', help='Job name', nargs='?')
    parser_stderr.set_defaults(func=command_stderr)

    # Version
    parser.add_argument('--version', action='version',
                        version='%(prog)s {}'.format(__version__),
                        help='show the version number and exit')

    # Print help if necessary
    if len(sys.argv) < 2:
        parser.print_help(sys.stderr)
        exit(1)

    # Parse the arguments & run the required function if necessary
    args = parser.parse_args()

    # Authentication
    if 'PROMINENCE_OIDC_URL' not in os.environ:
        print('Error: environment variable PROMINENCE_OIDC_URL is not set')
        exit(1)

    # Check that the URL for the PROMINENCE service exists
    if 'PROMINENCE_URL' not in os.environ:
        print('Error: Environment variable PROMINENCE_URL is not set')
        exit(1)

    # Run
    args.func(args)
     
